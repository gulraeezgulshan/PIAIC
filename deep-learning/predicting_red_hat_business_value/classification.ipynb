{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Hat Business Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the dataset provided for the “Red Hat Business Value” competition. This competition was hosted on Kaggle a few years back, and the dataset is a really good business use case for our study. The archived competition is available at www.kaggle.com/c/predicting-red-hat-business-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It highlights the problem that deals with predicting high-value customers for their business based on the operational interaction data and thereby helping the company effectively prioritize resources to generate more business and serve its customers better.\n",
    "\n",
    "* The organization is an American multinational software company that provides open source software products to the enterprise community. \n",
    "* Their primary product is Red Hat Enterprise Linux, the most popular distribution of Linux OS, used by various large enterprises. \n",
    "* In its services, it helps organizations align their IT strategies by providing enterprise-grade solutions through an open business model and an affordable, predictable subscription model. \n",
    "* These subscriptions from large enterprise customers create a substantial part of their revenue, and therefore it is of paramount importance for them to understand their valuable customers and serve them better by prioritizing resources and strategies to drive improved business value.\n",
    "\n",
    "Red Hat has been in existence for over 25 years. In the long stint of business, they have accumulated and captured a vast amount of data from customer interactions and their descriptive attributes. This rich source of data could be a gold mine of patterns that can help in identifying a potential customer by studying the vast and complex historical patterns in the interaction data.\n",
    "\n",
    "With the ever-growing popularity and prowess of DL, we can develop a DNN that can learn from historic customer attributes and operational interaction data to understand the deep patterns and predict whether a new customer will potentially be a high-value customer for various business services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the 2 datasets provided in the Zip Folder\n",
    "df = pd.read_csv(\"act_train.csv\")\n",
    "people = pd.read_csv(\"people.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Activities DF: (2197291, 15)\n",
      "Shape of People DF: (189118, 41)\n"
     ]
    }
   ],
   "source": [
    "#Explore the shape of the datasets\n",
    "print(\"Shape of Activities DF:\",df.shape)\n",
    "print(\"Shape of People DF:\",people.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people_id</th>\n",
       "      <th>activity_id</th>\n",
       "      <th>date</th>\n",
       "      <th>activity_category</th>\n",
       "      <th>char_1</th>\n",
       "      <th>char_2</th>\n",
       "      <th>char_3</th>\n",
       "      <th>char_4</th>\n",
       "      <th>char_5</th>\n",
       "      <th>char_6</th>\n",
       "      <th>char_7</th>\n",
       "      <th>char_8</th>\n",
       "      <th>char_9</th>\n",
       "      <th>char_10</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_1734928</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>type 4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_2434093</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>type 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_3404049</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>type 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_3651215</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>type 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_4109017</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>type 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  people_id   activity_id        date activity_category char_1 char_2 char_3  \\\n",
       "0   ppl_100  act2_1734928  2023-08-26            type 4    NaN    NaN    NaN   \n",
       "1   ppl_100  act2_2434093  2022-09-27            type 2    NaN    NaN    NaN   \n",
       "2   ppl_100  act2_3404049  2022-09-27            type 2    NaN    NaN    NaN   \n",
       "3   ppl_100  act2_3651215  2023-08-04            type 2    NaN    NaN    NaN   \n",
       "4   ppl_100  act2_4109017  2023-08-26            type 2    NaN    NaN    NaN   \n",
       "\n",
       "  char_4 char_5 char_6 char_7 char_8 char_9  char_10  outcome  \n",
       "0    NaN    NaN    NaN    NaN    NaN    NaN  type 76        0  \n",
       "1    NaN    NaN    NaN    NaN    NaN    NaN   type 1        0  \n",
       "2    NaN    NaN    NaN    NaN    NaN    NaN   type 1        0  \n",
       "3    NaN    NaN    NaN    NaN    NaN    NaN   type 1        0  \n",
       "4    NaN    NaN    NaN    NaN    NaN    NaN   type 1        0  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explore the contents of the first dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the contents of the training dataset, we can see that it mostly has customer interaction data but is completely anonymized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people_id            0.000000\n",
       "activity_id          0.000000\n",
       "date                 0.000000\n",
       "activity_category    0.000000\n",
       "char_1               0.928268\n",
       "char_2               0.928268\n",
       "char_3               0.928268\n",
       "char_4               0.928268\n",
       "char_5               0.928268\n",
       "char_6               0.928268\n",
       "char_7               0.928268\n",
       "char_8               0.928268\n",
       "char_9               0.928268\n",
       "char_10              0.071732\n",
       "outcome              0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the % of Null values in each column for activity data\n",
    "df.isnull().sum()/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Around nine features have more than 90% null values. We can’t do much to fix these features. [char_1 to char_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18514699"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#missing values/null values\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people_id</th>\n",
       "      <th>char_1</th>\n",
       "      <th>group_1</th>\n",
       "      <th>char_2</th>\n",
       "      <th>date</th>\n",
       "      <th>char_3</th>\n",
       "      <th>char_4</th>\n",
       "      <th>char_5</th>\n",
       "      <th>char_6</th>\n",
       "      <th>char_7</th>\n",
       "      <th>...</th>\n",
       "      <th>char_29</th>\n",
       "      <th>char_30</th>\n",
       "      <th>char_31</th>\n",
       "      <th>char_32</th>\n",
       "      <th>char_33</th>\n",
       "      <th>char_34</th>\n",
       "      <th>char_35</th>\n",
       "      <th>char_36</th>\n",
       "      <th>char_37</th>\n",
       "      <th>char_38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>2021-06-29</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 3</td>\n",
       "      <td>type 11</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ppl_100002</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 8688</td>\n",
       "      <td>type 3</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>type 28</td>\n",
       "      <td>type 9</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 3</td>\n",
       "      <td>type 11</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ppl_100003</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 33592</td>\n",
       "      <td>type 3</td>\n",
       "      <td>2022-06-10</td>\n",
       "      <td>type 4</td>\n",
       "      <td>type 8</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 2</td>\n",
       "      <td>type 5</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ppl_100004</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 22593</td>\n",
       "      <td>type 3</td>\n",
       "      <td>2022-07-20</td>\n",
       "      <td>type 40</td>\n",
       "      <td>type 25</td>\n",
       "      <td>type 9</td>\n",
       "      <td>type 4</td>\n",
       "      <td>type 16</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppl_100006</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 6534</td>\n",
       "      <td>type 3</td>\n",
       "      <td>2022-07-27</td>\n",
       "      <td>type 40</td>\n",
       "      <td>type 25</td>\n",
       "      <td>type 9</td>\n",
       "      <td>type 3</td>\n",
       "      <td>type 8</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    people_id  char_1      group_1  char_2        date   char_3   char_4  \\\n",
       "0     ppl_100  type 2  group 17304  type 2  2021-06-29   type 5   type 5   \n",
       "1  ppl_100002  type 2   group 8688  type 3  2021-01-06  type 28   type 9   \n",
       "2  ppl_100003  type 2  group 33592  type 3  2022-06-10   type 4   type 8   \n",
       "3  ppl_100004  type 2  group 22593  type 3  2022-07-20  type 40  type 25   \n",
       "4  ppl_100006  type 2   group 6534  type 3  2022-07-27  type 40  type 25   \n",
       "\n",
       "   char_5  char_6   char_7  ... char_29 char_30  char_31  char_32  char_33  \\\n",
       "0  type 5  type 3  type 11  ...   False    True     True    False    False   \n",
       "1  type 5  type 3  type 11  ...   False    True     True     True     True   \n",
       "2  type 5  type 2   type 5  ...   False   False     True     True     True   \n",
       "3  type 9  type 4  type 16  ...    True    True     True     True     True   \n",
       "4  type 9  type 3   type 8  ...   False   False     True    False    False   \n",
       "\n",
       "   char_34  char_35  char_36  char_37  char_38  \n",
       "0     True     True     True    False       36  \n",
       "1     True     True     True    False       76  \n",
       "2     True    False     True     True       99  \n",
       "3     True     True     True     True       76  \n",
       "4    False     True     True    False       84  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explore the contents of People dataset\n",
    "people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the % of null values in for the entire dataset\n",
    "people.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And we see that none of the columns in the customer dataset has missing values. But before we do that, we need to take care of a few things as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To create a consolidated dataset, we need to join the activity and customer data on the people_id key. \n",
    "* We need to drop the columns in the activity data that have 90% missing values, as they cannot be fixed. [char_1 to char_9]\n",
    "* Secondly, the “date” and “char_10” columns are present in both datasets. \n",
    "* In order to avoid a nameclash, let us rename the “date” column in the activity dataset to “activity_date” and “char_10” in the activity data as “activity_type.” \n",
    "* Next, we also need to fix the missing values in the “activity_type” column. \n",
    "* Once these two tasks are accomplished, we will join the two datasets and explore the consolidated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to remove: ['char_1', 'char_2', 'char_3', 'char_4', 'char_5', 'char_6', 'char_7', 'char_8', 'char_9']\n"
     ]
    }
   ],
   "source": [
    "#Create the list of columns to drop from activity data\n",
    "columns_to_remove = [\"char_\"+str(x) for x in np.arange(1,10)]\n",
    "print(\"Columns to remove:\",columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the columns from the activity data\n",
    "df = df[list(set(df.columns) - set(columns_to_remove))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "      <th>char_10</th>\n",
       "      <th>people_id</th>\n",
       "      <th>date</th>\n",
       "      <th>activity_category</th>\n",
       "      <th>activity_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>type 76</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>type 4</td>\n",
       "      <td>act2_1734928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>type 1</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>type 2</td>\n",
       "      <td>act2_2434093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>type 1</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>type 2</td>\n",
       "      <td>act2_3404049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>type 1</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>type 2</td>\n",
       "      <td>act2_3651215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>type 1</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>type 2</td>\n",
       "      <td>act2_4109017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   outcome  char_10 people_id        date activity_category   activity_id\n",
       "0        0  type 76   ppl_100  2023-08-26            type 4  act2_1734928\n",
       "1        0   type 1   ppl_100  2022-09-27            type 2  act2_2434093\n",
       "2        0   type 1   ppl_100  2022-09-27            type 2  act2_3404049\n",
       "3        0   type 1   ppl_100  2023-08-04            type 2  act2_3651215\n",
       "4        0   type 1   ppl_100  2023-08-26            type 2  act2_4109017"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the 2 columns to avoid name clashes in merged data\n",
    "df = df.rename(columns={\"date\":\"activity_date\",\"char_10\":\"activity_type\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "      <th>activity_type</th>\n",
       "      <th>people_id</th>\n",
       "      <th>activity_date</th>\n",
       "      <th>activity_category</th>\n",
       "      <th>activity_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>type 76</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>type 4</td>\n",
       "      <td>act2_1734928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>type 1</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>type 2</td>\n",
       "      <td>act2_2434093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>type 1</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>type 2</td>\n",
       "      <td>act2_3404049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>type 1</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>type 2</td>\n",
       "      <td>act2_3651215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>type 1</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>type 2</td>\n",
       "      <td>act2_4109017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   outcome activity_type people_id activity_date activity_category  \\\n",
       "0        0       type 76   ppl_100    2023-08-26            type 4   \n",
       "1        0        type 1   ppl_100    2022-09-27            type 2   \n",
       "2        0        type 1   ppl_100    2022-09-27            type 2   \n",
       "3        0        type 1   ppl_100    2023-08-04            type 2   \n",
       "4        0        type 1   ppl_100    2023-08-26            type 2   \n",
       "\n",
       "    activity_id  \n",
       "0  act2_1734928  \n",
       "1  act2_2434093  \n",
       "2  act2_3404049  \n",
       "3  act2_3651215  \n",
       "4  act2_4109017  "
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    type 1\n",
       "dtype: object"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The mode of a set of data values is the value that appears most often.\n",
    "df[\"activity_type\"].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace nulls in the activity_type column with the mode\n",
    "df[\"activity_type\"] = df[\"activity_type\"].fillna(df[\"activity_type\"].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Shape: (2197291, 15)\n",
      "New Shape: (2197291, 6)\n"
     ]
    }
   ],
   "source": [
    "#Print the shape of the final activity dataset\n",
    "print(\"Old Shape:\",\"(2197291, 15)\")\n",
    "print(\"New Shape:\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the 2 datasets on 'people_id' key\n",
    "df_new = df.merge(people,on=[\"people_id\"],how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "      <th>activity_type</th>\n",
       "      <th>people_id</th>\n",
       "      <th>activity_date</th>\n",
       "      <th>activity_category</th>\n",
       "      <th>activity_id</th>\n",
       "      <th>char_1</th>\n",
       "      <th>group_1</th>\n",
       "      <th>char_2</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>char_29</th>\n",
       "      <th>char_30</th>\n",
       "      <th>char_31</th>\n",
       "      <th>char_32</th>\n",
       "      <th>char_33</th>\n",
       "      <th>char_34</th>\n",
       "      <th>char_35</th>\n",
       "      <th>char_36</th>\n",
       "      <th>char_37</th>\n",
       "      <th>char_38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>type 76</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>type 4</td>\n",
       "      <td>act2_1734928</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>2021-06-29</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>type 1</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>type 2</td>\n",
       "      <td>act2_2434093</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>2021-06-29</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>type 1</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>type 2</td>\n",
       "      <td>act2_3404049</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>2021-06-29</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>type 1</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>type 2</td>\n",
       "      <td>act2_3651215</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>2021-06-29</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>type 1</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>type 2</td>\n",
       "      <td>act2_4109017</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>2021-06-29</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   outcome activity_type people_id activity_date activity_category  \\\n",
       "0        0       type 76   ppl_100    2023-08-26            type 4   \n",
       "1        0        type 1   ppl_100    2022-09-27            type 2   \n",
       "2        0        type 1   ppl_100    2022-09-27            type 2   \n",
       "3        0        type 1   ppl_100    2023-08-04            type 2   \n",
       "4        0        type 1   ppl_100    2023-08-26            type 2   \n",
       "\n",
       "    activity_id  char_1      group_1  char_2        date  ... char_29 char_30  \\\n",
       "0  act2_1734928  type 2  group 17304  type 2  2021-06-29  ...   False    True   \n",
       "1  act2_2434093  type 2  group 17304  type 2  2021-06-29  ...   False    True   \n",
       "2  act2_3404049  type 2  group 17304  type 2  2021-06-29  ...   False    True   \n",
       "3  act2_3651215  type 2  group 17304  type 2  2021-06-29  ...   False    True   \n",
       "4  act2_4109017  type 2  group 17304  type 2  2021-06-29  ...   False    True   \n",
       "\n",
       "  char_31 char_32 char_33 char_34 char_35  char_36  char_37  char_38  \n",
       "0    True   False   False    True    True     True    False       36  \n",
       "1    True   False   False    True    True     True    False       36  \n",
       "2    True   False   False    True    True     True    False       36  \n",
       "3    True   False   False    True    True     True    False       36  \n",
       "4    True   False   False    True    True     True    False       36  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before merging: (2197291, 6)\n",
      "Shape after merging : (2197291, 46)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape before merging:\",df.shape)\n",
    "print(\"Shape after merging :\",df_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The consistent number of rows and the increase in the number of columns helps us validate that the join operation worked as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now study the target (i.e., the variable we want to predict), named “outcome” in the dataset. \n",
    "We can check the distribution between potential vs. nonpotential customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for outcome: [0 1]\n",
      "\n",
      "Percentage of distribution for outcome-\n",
      "0    0.556046\n",
      "1    0.443954\n",
      "Name: outcome, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values for outcome:\",df_new[\"outcome\"].unique())\n",
    "print(\"\\nPercentage of distribution for outcome-\")\n",
    "print(df_new[\"outcome\"].value_counts()/df_new.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see that there is a good mix in the distribution of potentialcustomers, as around 45% are potential customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct DataTypes: [dtype('int64'), dtype('O'), dtype('bool')]\n"
     ]
    }
   ],
   "source": [
    "#Checking the distinct datatypes in the dataset\n",
    "print(\"Distinct DataTypes:\",list(df_new.dtypes.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have numeric, categorical (Object), and Boolean features in the dataset. \n",
    "* Boolean in Python represents a True or False value; \n",
    "* We need to convert this into numeric (1 and 0) for the model to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a temp dataset with the datatype of columns\n",
    "temp = pd.DataFrame(df_new.dtypes)\n",
    "temp.columns = [\"DataType\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>outcome</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_type</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people_id</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_date</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_category</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_id</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_1</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_1</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_2</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_3</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_4</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_5</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_6</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_7</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_8</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_9</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_10</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_11</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_12</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_13</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_14</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_15</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_16</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_17</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_18</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_19</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_20</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_21</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_22</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_23</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_24</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_25</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_26</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_27</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_28</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_29</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_30</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_31</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_32</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_33</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_34</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_35</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_36</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_37</th>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_38</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  DataType\n",
       "outcome              int64\n",
       "activity_type       object\n",
       "people_id           object\n",
       "activity_date       object\n",
       "activity_category   object\n",
       "activity_id         object\n",
       "char_1              object\n",
       "group_1             object\n",
       "char_2              object\n",
       "date                object\n",
       "char_3              object\n",
       "char_4              object\n",
       "char_5              object\n",
       "char_6              object\n",
       "char_7              object\n",
       "char_8              object\n",
       "char_9              object\n",
       "char_10               bool\n",
       "char_11               bool\n",
       "char_12               bool\n",
       "char_13               bool\n",
       "char_14               bool\n",
       "char_15               bool\n",
       "char_16               bool\n",
       "char_17               bool\n",
       "char_18               bool\n",
       "char_19               bool\n",
       "char_20               bool\n",
       "char_21               bool\n",
       "char_22               bool\n",
       "char_23               bool\n",
       "char_24               bool\n",
       "char_25               bool\n",
       "char_26               bool\n",
       "char_27               bool\n",
       "char_28               bool\n",
       "char_29               bool\n",
       "char_30               bool\n",
       "char_31               bool\n",
       "char_32               bool\n",
       "char_33               bool\n",
       "char_34               bool\n",
       "char_35               bool\n",
       "char_36               bool\n",
       "char_37               bool\n",
       "char_38              int64"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean columns - \n",
      " ['char_10' 'char_11' 'char_12' 'char_13' 'char_14' 'char_15' 'char_16'\n",
      " 'char_17' 'char_18' 'char_19' 'char_20' 'char_21' 'char_22' 'char_23'\n",
      " 'char_24' 'char_25' 'char_26' 'char_27' 'char_28' 'char_29' 'char_30'\n",
      " 'char_31' 'char_32' 'char_33' 'char_34' 'char_35' 'char_36' 'char_37']\n"
     ]
    }
   ],
   "source": [
    "#Create a list with names of all Boolean columns\n",
    "boolean_columns = temp.index[temp[\"DataType\"] == 'bool'].values\n",
    "print(\"Boolean columns - \\n\",boolean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distinct DataTypes after processing: [dtype('int64') dtype('O')]\n"
     ]
    }
   ],
   "source": [
    "#Convert all boolean columns to Binary numeric values\n",
    "for column in boolean_columns:\n",
    "    df_new[column] = np.where(df_new[column] == True,1,0)\n",
    "\n",
    "print(\"\\nDistinct DataTypes after processing:\",df.dtypes.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the object columns from the above dataframe\n",
    "categorical_columns = temp.index[temp[\"DataType\"] == 'O'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['activity_type', 'people_id', 'activity_date', 'activity_category',\n",
       "       'activity_id', 'char_1', 'group_1', 'char_2', 'date', 'char_3',\n",
       "       'char_4', 'char_5', 'char_6', 'char_7', 'char_8', 'char_9'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activity_type column has : 6515 distinct values\n",
      "people_id column has : 151295 distinct values\n",
      "activity_date column has : 411 distinct values\n",
      "activity_category column has : 7 distinct values\n",
      "activity_id column has : 2197291 distinct values\n",
      "char_1 column has : 2 distinct values\n",
      "group_1 column has : 29899 distinct values\n",
      "char_2 column has : 3 distinct values\n",
      "date column has : 1196 distinct values\n",
      "char_3 column has : 43 distinct values\n",
      "char_4 column has : 25 distinct values\n",
      "char_5 column has : 9 distinct values\n",
      "char_6 column has : 7 distinct values\n",
      "char_7 column has : 25 distinct values\n",
      "char_8 column has : 8 distinct values\n",
      "char_9 column has : 9 distinct values\n"
     ]
    }
   ],
   "source": [
    "#Check the number of distinct values in each categorical column\n",
    "for column in categorical_columns:\n",
    "    print(column+\" column has :\",str(len(df_new[column].unique()))+\" distinct values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The five highlighted columns as shown in the output have high numbers of distinct values. It would be difficult to convert them into a one-hot encoded form, as they will consume too much memory during processing. In case you have the luxury of surplus RAM, feel free to convert them to a one-hot encoded data form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data after create Date Features: (2197291, 56)\n"
     ]
    }
   ],
   "source": [
    "#Create date related features for 'date' in customer data\n",
    "df_new[\"date\"] = pd.to_datetime(df_new[\"date\"])\n",
    "\n",
    "df_new[\"Year\"] = df_new[\"date\"].dt.year\n",
    "df_new[\"Month\"] = df_new[\"date\"].dt.month\n",
    "df_new[\"Quarter\"] = df_new[\"date\"].dt.quarter\n",
    "df_new[\"Week\"] = df_new[\"date\"].dt.week\n",
    "df_new[\"WeekDay\"] = df_new[\"date\"].dt.weekday\n",
    "df_new[\"Day\"] = df_new[\"date\"].dt.day\n",
    "\n",
    "#Create date related features for 'date' in activity data\n",
    "df_new[\"activity_date\"] = pd.to_datetime(df_new[\"activity_date\"])\n",
    "\n",
    "df_new[\"Activity_Year\"] = df_new[\"activity_date\"].dt.year\n",
    "df_new[\"Activity_Month\"] = df_new[\"activity_date\"].dt.month\n",
    "df_new[\"Activity_Quarter\"] = df_new[\"activity_date\"].dt.quarter\n",
    "df_new[\"Activity_Week\"] = df_new[\"activity_date\"].dt.week\n",
    "df_new[\"Activity_WeekDay\"] = df_new[\"activity_date\"].dt.weekday\n",
    "df_new[\"Activity_Day\"] = df_new[\"activity_date\"].dt.day\n",
    "\n",
    "#Delete the original date columns\n",
    "del(df_new[\"date\"])\n",
    "del(df_new[\"activity_date\"])\n",
    "\n",
    "print(\"Shape of data after create Date Features:\",df_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  people_id activity_type   activity_id      group_1\n",
      "0   ppl_100       type 76  act2_1734928  group 17304\n",
      "1   ppl_100        type 1  act2_2434093  group 17304\n",
      "2   ppl_100        type 1  act2_3404049  group 17304\n",
      "3   ppl_100        type 1  act2_3651215  group 17304\n",
      "4   ppl_100        type 1  act2_4109017  group 17304\n"
     ]
    }
   ],
   "source": [
    "print(df_new[[\"people_id\",\"activity_type\",\"activity_id\",\"group_1\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It seems that we can convert all of the preceding categorical columns into numeric by extracting the relevant numeric ID from each of them.\n",
    "* Since each of these columns has values in the form of someText_ someNumber. \n",
    "* Rather than converting these categorical columns into a bloated one-hot encoded dataset, we can temporarily use them as numeric features. \n",
    "* However, if the performance of the model doesn’t reach our desired expectations after several experiments, we might have to revisit these features and try our best to incorporate them differently. But for now, we can consider them as numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   people_id  activity_type  activity_id  group_1\n",
      "0      100.0             76    1734928.0    17304\n",
      "1      100.0              1    2434093.0    17304\n",
      "2      100.0              1    3404049.0    17304\n",
      "3      100.0              1    3651215.0    17304\n",
      "4      100.0              1    4109017.0    17304\n"
     ]
    }
   ],
   "source": [
    "#For people ID, we would need to extract values after '_'\n",
    "df_new.people_id = df_new.people_id.apply(lambda x:x.split(\"_\")[1])\n",
    "df_new.people_id = pd.to_numeric(df_new.people_id)\n",
    "\n",
    "#For activity ID also, we would need to extract values after '_'\n",
    "df_new.activity_id = df_new.activity_id.apply(lambda x:x.split(\"_\")[1])\n",
    "df_new.activity_id = pd.to_numeric(df_new.activity_id)\n",
    "\n",
    "#For group_1 , we would need to extract values after ' '\n",
    "df_new.group_1 = df_new.group_1.apply(lambda x:x.split(\" \")[1])\n",
    "df_new.group_1 = pd.to_numeric(df_new.group_1)\n",
    "\n",
    "#For activity_type , we would need to extract values after ' '\n",
    "df_new.activity_type = df_new.activity_type.apply(lambda x:x.split(\" \")[1])\n",
    "df_new.activity_type = pd.to_numeric(df_new.activity_type)\n",
    "\n",
    "#Double check the new values in the dataframe\n",
    "print(df_new[[\"people_id\",\"activity_type\",\"activity_id\",\"group_1\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s convert the remaining categorical columns, which have relatively low numbers of distinct values, to one-hot encoded form and render the final consolidated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "#Creating a temporary dataframe that hosts column datatypes\n",
    "temp = pd.DataFrame(df_new.dtypes)\n",
    "temp.columns = [\"DataType\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>outcome</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_type</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people_id</th>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_category</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_id</th>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_1</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_1</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_2</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_3</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_4</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_5</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_6</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_7</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_8</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_9</th>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_10</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_11</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_12</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_13</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_14</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_15</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_16</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_17</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_18</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_19</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_20</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_21</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_22</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_23</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_24</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_25</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_26</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_27</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_28</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_29</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_30</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_31</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_32</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_33</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_34</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_35</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_36</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_37</th>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_38</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quarter</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Week</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeekDay</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Activity_Year</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Activity_Month</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Activity_Quarter</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Activity_Week</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Activity_WeekDay</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Activity_Day</th>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  DataType\n",
       "outcome              int64\n",
       "activity_type        int64\n",
       "people_id          float64\n",
       "activity_category   object\n",
       "activity_id        float64\n",
       "char_1              object\n",
       "group_1              int64\n",
       "char_2              object\n",
       "char_3              object\n",
       "char_4              object\n",
       "char_5              object\n",
       "char_6              object\n",
       "char_7              object\n",
       "char_8              object\n",
       "char_9              object\n",
       "char_10              int32\n",
       "char_11              int32\n",
       "char_12              int32\n",
       "char_13              int32\n",
       "char_14              int32\n",
       "char_15              int32\n",
       "char_16              int32\n",
       "char_17              int32\n",
       "char_18              int32\n",
       "char_19              int32\n",
       "char_20              int32\n",
       "char_21              int32\n",
       "char_22              int32\n",
       "char_23              int32\n",
       "char_24              int32\n",
       "char_25              int32\n",
       "char_26              int32\n",
       "char_27              int32\n",
       "char_28              int32\n",
       "char_29              int32\n",
       "char_30              int32\n",
       "char_31              int32\n",
       "char_32              int32\n",
       "char_33              int32\n",
       "char_34              int32\n",
       "char_35              int32\n",
       "char_36              int32\n",
       "char_37              int32\n",
       "char_38              int64\n",
       "Year                 int64\n",
       "Month                int64\n",
       "Quarter              int64\n",
       "Week                 int64\n",
       "WeekDay              int64\n",
       "Day                  int64\n",
       "Activity_Year        int64\n",
       "Activity_Month       int64\n",
       "Activity_Quarter     int64\n",
       "Activity_Week        int64\n",
       "Activity_WeekDay     int64\n",
       "Activity_Day         int64"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists to separate numeric, categorical and target features\n",
    "target = [\"outcome\"]\n",
    "numeric_columns = list(set(temp.index[(temp.DataType ==\"float64\") | (temp.DataType ==\"int64\")].values) - set(target))\n",
    "categorical_columns = temp.index[temp.DataType == \"O\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['activity_category', 'char_1', 'char_2', 'char_3', 'char_4',\n",
       "       'char_5', 'char_6', 'char_7', 'char_8', 'char_9'], dtype=object)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target column :\n",
      " ['outcome']\n",
      "\n",
      "Numeric columns :\n",
      " ['Month', 'activity_type', 'Activity_WeekDay', 'Activity_Week', 'char_38', 'Activity_Day', 'Quarter', 'Day', 'people_id', 'Week', 'WeekDay', 'group_1', 'Activity_Quarter', 'Activity_Year', 'activity_id', 'Activity_Month', 'Year']\n",
      "\n",
      "Categorical columns :\n",
      " ['activity_category' 'char_1' 'char_2' 'char_3' 'char_4' 'char_5' 'char_6'\n",
      " 'char_7' 'char_8' 'char_9']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTarget column :\\n\",target)\n",
    "print(\"\\nNumeric columns :\\n\",numeric_columns)\n",
    "print(\"\\nCategorical columns :\\n\",categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that will intake the raw dataframe and the column name and return a one hot encoded DF\n",
    "def create_ohe(df, col):\n",
    "    le = LabelEncoder()\n",
    "    a=le.fit_transform(df_new[col]).reshape(-1,1)\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    column_names = [col+ \"_\"+ str(i) for i in le.classes_]\n",
    "    return(pd.DataFrame(ohe.fit_transform(a),columns =column_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of final df after onehot encoding: (2197291, 155)\n"
     ]
    }
   ],
   "source": [
    "#Since the above function coverts the column, one at a time\n",
    "#We create a loop to create the final dataset with all features\n",
    "temp = df_new[numeric_columns]\n",
    "for column in categorical_columns:\n",
    "    temp_df = create_ohe(df_new,column)\n",
    "    temp = pd.concat([temp,temp_df],axis=1)\n",
    "\n",
    "print(\"\\nShape of final df after onehot encoding:\",temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (1582048, 155)\n",
      "Shape of x_test: (439459, 155)\n",
      "Shape of x_val: (175784, 155)\n",
      "Shape of y_train: (1582048, 1)\n",
      "Shape of y_test: (439459, 1)\n",
      "Shape of y_val: (175784, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split the final dataset into train and test with 80:20\n",
    "x_train, x_test, y_train, y_test = train_test_split(temp,df_new[target], test_size=0.2,random_state=2018)\n",
    "\n",
    "#split the train dataset further into train and validation with 90:10\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train,y_train,test_size=0.1,random_state=2018)\n",
    "\n",
    "#Check the shape of each new dataset created\n",
    "print(\"Shape of x_train:\",x_train.shape)\n",
    "print(\"Shape of x_test:\",x_test.shape)\n",
    "print(\"Shape of x_val:\",x_val.shape)\n",
    "print(\"Shape of y_train:\",y_train.shape)\n",
    "print(\"Shape of y_test:\",y_test.shape)\n",
    "print(\"Shape of y_val:\",y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing a Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 1582048 samples, validate on 175784 samples\n",
      "Epoch 1/3\n",
      "1582048/1582048 [==============================] - 135s 85us/step - loss: 7.1534 - acc: 0.5562 - val_loss: 7.1813 - val_acc: 0.5545\n",
      "Epoch 2/3\n",
      "1582048/1582048 [==============================] - 134s 85us/step - loss: 7.1534 - acc: 0.5562 - val_loss: 7.1813 - val_acc: 0.5545\n",
      "Epoch 3/3\n",
      "1582048/1582048 [==============================] - 133s 84us/step - loss: 7.1534 - acc: 0.5562 - val_loss: 7.1813 - val_acc: 0.5545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x241bd6816cf8>"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#Design the deep neural network [Small + 1 layer]\n",
    "model  = Sequential()\n",
    "model.add(Dense(256,input_dim = x_train.shape[1],activation=\"relu\"))\n",
    "model.add(Dense(256,activation=\"relu\"))\n",
    "model.add(Dense(1,activation = \"sigmoid\")) #activation = sigmoid for binary classification\n",
    "\n",
    "model.compile(optimizer = \"Adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train,y_train, validation_data = (x_val,y_val),epochs=3, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1582048 samples, validate on 175784 samples\n",
      "Epoch 1/3\n",
      "1582048/1582048 [==============================] - 124s 78us/step - loss: 7.1534 - acc: 0.5562 - val_loss: 7.1813 - val_acc: 0.5545\n",
      "Epoch 2/3\n",
      "1582048/1582048 [==============================] - 125s 79us/step - loss: 7.1534 - acc: 0.5562 - val_loss: 7.1813 - val_acc: 0.5545\n",
      "Epoch 3/3\n",
      "1582048/1582048 [==============================] - 127s 80us/step - loss: 7.1534 - acc: 0.5562 - val_loss: 7.1813 - val_acc: 0.5545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x46764009fef0>"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Design the deep neural network [Small + 2 layers]\n",
    "model  = Sequential()\n",
    "model.add(Dense(256,input_dim = x_train.shape[1],activation=\"relu\"))\n",
    "model.add(Dense(256,activation=\"relu\"))\n",
    "model.add(Dense(1,activation = \"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer = \"Adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train,y_train, validation_data = (x_val,y_val),epochs=3, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1582048 samples, validate on 175784 samples\n",
      "Epoch 1/3\n",
      "1582048/1582048 [==============================] - 113s 71us/step - loss: 8.3515 - acc: 0.4776 - val_loss: 8.8394 - val_acc: 0.4455\n",
      "Epoch 2/3\n",
      "1582048/1582048 [==============================] - 117s 74us/step - loss: 8.8669 - acc: 0.4438 - val_loss: 8.8394 - val_acc: 0.4455\n",
      "Epoch 3/3\n",
      "1582048/1582048 [==============================] - 114s 72us/step - loss: 8.8669 - acc: 0.4438 - val_loss: 8.8394 - val_acc: 0.4455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x241becaaffd0>"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Design the deep neural network [Medium + 1 layers]\n",
    "model  = Sequential()\n",
    "model.add(Dense(512,input_dim = x_train.shape[1],activation=\"relu\"))\n",
    "model.add(Dense(1,activation = \"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer = \"Adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train,y_train, validation_data = (x_val,y_val),epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1582048 samples, validate on 175784 samples\n",
      "Epoch 1/3\n",
      "1582048/1582048 [==============================] - 209s 132us/step - loss: 8.8426 - acc: 0.4454 - val_loss: 8.8394 - val_acc: 0.4455\n",
      "Epoch 2/3\n",
      "1582048/1582048 [==============================] - 209s 132us/step - loss: 8.8669 - acc: 0.4438 - val_loss: 8.8394 - val_acc: 0.4455\n",
      "Epoch 3/3\n",
      "1582048/1582048 [==============================] - 207s 131us/step - loss: 8.8669 - acc: 0.4438 - val_loss: 8.8394 - val_acc: 0.4455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x46762a5b5ba8>"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Design the deep neural network [Medium + 2 layers]\n",
    "model  = Sequential()\n",
    "model.add(Dense(512,input_dim = x_train.shape[1],activation=\"relu\"))\n",
    "model.add(Dense(512,activation=\"relu\"))\n",
    "model.add(Dense(1,activation = \"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer = \"Adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train,y_train, validation_data = (x_val,y_val),epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisiting the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Input =  [1 2 3 4 5 6 7 8 9]\n",
      "Output =\n",
      "  [array([-1.54919334]), array([-1.161895]), array([-0.77459667]), array([-0.38729833]), array([0.]), array([0.38729833]), array([0.77459667]), array([1.161895]), array([1.54919334])]\n",
      "Output's Mean =  0.0\n",
      "Output's Std Dev =  1.0\n",
      "\n",
      "After Inverse Transforming = \n",
      " [array([1.]), array([2.]), array([3.]), array([4.]), array([5.]), array([6.]), array([7.]), array([8.]), array([9.])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#Create a dummy input\n",
    "dummy_input = np.arange(1,10)\n",
    "print(\"Dummy Input = \",dummy_input)\n",
    "\n",
    "from sklearn.preprocessing  import StandardScaler\n",
    "\n",
    "#Create a standardscaler instance and fit the data\n",
    "scaler = StandardScaler()\n",
    "output = scaler.fit_transform(dummy_input.reshape(-1,1))\n",
    "\n",
    "print(\"Output =\\n \",list(output))\n",
    "print(\"Output's Mean = \",output.mean())\n",
    "print(\"Output's Std Dev = \",output.std())\n",
    "print(\"\\nAfter Inverse Transforming = \\n\",list(scaler.inverse_transform(output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n",
      "C:\\Users\\Gulraiz\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "x_test_scaled = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks for Classification with improved data\n",
    "Let us now, start with a medium sized network to see if we get improved results. We would start with just 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1582048 samples, validate on 175784 samples\n",
      "Epoch 1/3\n",
      "1582048/1582048 [==============================] - 114s 72us/step - loss: 0.2521 - acc: 0.8891 - val_loss: 0.2179 - val_acc: 0.9093\n",
      "Epoch 2/3\n",
      "1582048/1582048 [==============================] - 116s 73us/step - loss: 0.2020 - acc: 0.9179 - val_loss: 0.1905 - val_acc: 0.9253\n",
      "Epoch 3/3\n",
      "1582048/1582048 [==============================] - 114s 72us/step - loss: 0.1782 - acc: 0.9308 - val_loss: 0.1752 - val_acc: 0.9333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x467650e12a90>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "model  = Sequential()\n",
    "model.add(Dense(512,input_dim = x_train_scaled.shape[1],activation=\"relu\"))\n",
    "model.add(Dense(1,activation = \"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer = \"Adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train_scaled,y_train, \n",
    "validation_data = (x_val_scaled,y_val),epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439459/439459 [==============================] - 14s 32us/step\n",
      "Metric  loss : 0.18\n",
      "Metric  acc : 0.93\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x_test_scaled,y_test)\n",
    "for i in range(len(model.metrics_names)):\n",
    "    print(\"Metric \",model.metrics_names[i],\":\",str(round(result[i],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1582048 samples, validate on 175784 samples\n",
      "Epoch 1/3\n",
      "1582048/1582048 [==============================] - 220s 139us/step - loss: 0.2168 - acc: 0.9050 - val_loss: 0.1620 - val_acc: 0.9318\n",
      "Epoch 2/3\n",
      "1582048/1582048 [==============================] - 296s 187us/step - loss: 0.1405 - acc: 0.9431 - val_loss: 0.1283 - val_acc: 0.9486\n",
      "Epoch 3/3\n",
      "1582048/1582048 [==============================] - 286s 181us/step - loss: 0.1116 - acc: 0.9559 - val_loss: 0.1090 - val_acc: 0.9579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x467663868d68>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Designing the Deep Neural Network [Medium – 2 Layers]\n",
    "model  = Sequential()\n",
    "model.add(Dense(512,input_dim = x_train_scaled.shape[1],activation=\"relu\"))\n",
    "model.add(Dense(512,activation=\"relu\"))\n",
    "model.add(Dense(1,activation = \"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer = \"Adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train_scaled,y_train, validation_data = (x_val_scaled,y_val),epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Designing the network Deep Neural Network – [Large + 2 Layers]\n",
    "model  = Sequential()\n",
    "model.add(Dense(1024,input_dim = x_train_scaled.shape[1],activation=\"relu\"))\n",
    "model.add(Dense(1024,activation = \"relu\"))\n",
    "model.add(Dense(1,activation = \"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer = \"Adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train_scaled,y_train, \n",
    "validation_data = (x_val_scaled,y_val),epochs=3, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Designing the network Deep Neural Network – [Medium + 2 Layers]\n",
    "model  = Sequential()\n",
    "model.add(Dense(512,input_dim = x_train_scaled.shape[1],activation=\"relu\"))\n",
    "model.add(Dense(512,activation = \"relu\"))\n",
    "model.add(Dense(1,activation = \"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer = \"Adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train_scaled,y_train, validation_data = (x_val_scaled,y_val),epochs=15, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(x_test_scaled,y_test)\n",
    "\n",
    "for i in range(len(model.metrics_names)):\n",
    "    print(\"Metric \",model.metrics_names[i],\":\",str(round(result[i],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(model.history.history['loss'])\n",
    "plt.plot(model.history.history['val_loss'])\n",
    "plt.title(\"Model's Training & Validation loss across epochs\")\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.history.history['acc'])\n",
    "plt.plot(model.history.history['val_acc'])\n",
    "plt.title(\"Model's Training & Validation Accuracy across epochs\")\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
